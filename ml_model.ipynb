{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import sqlite3 as sql3\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import keras_tuner as kt\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data from sqlite file\n",
    "database_path = './FlaskSQLBackend/laptop_data_database.sqlite'\n",
    "\n",
    "laptop_df = ''\n",
    "\n",
    "with sql3.connect(database_path) as conn:\n",
    "    laptop_df = pd.read_sql(\"SELECT * FROM Laptops\", conn)\n",
    "\n",
    "    laptop_df = pd.get_dummies(laptop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "print(len(laptop_df.columns))\n",
    "\n",
    "X = laptop_df.drop('PRICE', axis=1)  # Assuming 'PRICE' is the target variable\n",
    "y = laptop_df['PRICE']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test size as needed\n",
    "\n",
    "# Scale numerical features using StandardScaler\n",
    "numerical_features = ['INCHES', 'MEMORY', 'WEIGHT', 'SCREENWIDTH', 'SCREENHEIGHT', 'CPUSPEED', 'PRIMARYSTORAGEAMOUNT', 'SECONDARYSTORAGEAMOUNT']\n",
    "X[numerical_features].apply(pd.to_numeric)\n",
    "X = X.dropna(how='any',axis=0) \n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "X_train[numerical_features] = X_scaler.fit_transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = X_scaler.transform(X_test[numerical_features]) # Use the same scaler fitted on training data\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train = y_scaler.fit_transform(pd.DataFrame(y_train))\n",
    "y_test = y_scaler.transform(pd.DataFrame(y_test)) # Use the same scaler fitted on training data\n",
    "\n",
    "X_train = X_train.to_numpy().astype(np.float32)\n",
    "X_test = X_test.to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 62)                3906      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 74)                4662      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                3750      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,369\n",
      "Trainable params: 12,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(hp):\n",
    "\t# Creating the model\n",
    "\tnn_model = tf.keras.models.Sequential()\n",
    "\n",
    "\t# Allowing the kerastuner to decide which activation function to use in the hidden layers\n",
    "\tactivation = hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "\n",
    "\tnn_model.add(tf.keras.layers.Dense(\n",
    "\t\tunits = hp.Int(\n",
    "\t\t\t'first_units',\n",
    "\t\t\tmin_value=30,\n",
    "\t\t\tmax_value=90,\n",
    "\t\t\tstep=2\n",
    "\t\t),\n",
    "\t\tactivation = activation,\n",
    "\t\tinput_dim=62\n",
    "\t))\n",
    "\n",
    "\tfor i in range(hp.Int('num_layers', 1, 5)):\n",
    "\t\tnn_model.add(tf.keras.layers.Dense(\n",
    "\t\t\tunits = hp.Int(\n",
    "\t\t\t\t'units_' + str(i),\n",
    "\t\t\t\tmin_value=10,\n",
    "\t\t\t\tmax_value=80,\n",
    "\t\t\t\tstep=2\n",
    "\t\t\t),\n",
    "\t\t\tactivation = activation\n",
    "\t\t))\n",
    "\n",
    "\tnn_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "\tnn_model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "\treturn nn_model\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "\tcreate_model,\n",
    "\tobjective='val_loss', # Validation accuracy\n",
    "\tmax_epochs=80,\n",
    "\thyperband_iterations=2\n",
    ")\n",
    "\n",
    "# Tuning for the best hyperparameters\n",
    "tuner.search(X_train, y_train, epochs=80, validation_data=(X_test, y_test))\n",
    "\n",
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "nn_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "nn_model.summary()\n",
    "\n",
    "nn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 940us/step\n",
      "Loss: 0.04130559787154198\n",
      "Mean Absolute Error: 0.11579518020153046\n",
      "R Squared: 0.9583918837350499\n",
      "        Predicted       Actual\n",
      "0    93363.000000   93985.9200\n",
      "1    58811.542969   58021.9200\n",
      "2    23158.441406   17529.1200\n",
      "3   105909.710938  101178.7200\n",
      "4    38182.933594   38041.3872\n",
      "5    81886.632812   84129.1200\n",
      "6    35854.910156   53226.7200\n",
      "7    62690.128906   63669.6000\n",
      "8    63510.132812   63882.7200\n",
      "9    55029.093750   53759.5200\n",
      "10   14852.277344   15557.7600\n",
      "11   47472.074219   53226.7200\n",
      "12   68236.515625   69210.7200\n",
      "13   30542.863281   35111.5200\n",
      "14   18882.730469   18328.3200\n",
      "15   80083.671875   93240.0000\n",
      "16   43511.628906   42357.6000\n",
      "17   61706.699219   76030.5600\n",
      "18   32990.492188   34578.7200\n",
      "19   56677.648438   58021.9200\n",
      "20   28422.984375   30316.3200\n",
      "21   18455.214844   18328.3200\n",
      "22   46629.316406   47898.7200\n",
      "23   19700.066406   17582.4000\n",
      "24   21338.730469   20459.5200\n",
      "25   50091.695312   50562.7200\n",
      "26  139185.781250  146519.4672\n",
      "27   42552.890625   43156.8000\n",
      "28   60691.039062   63159.7104\n",
      "29   78134.468750   88924.3200\n",
      "30   99894.273438  122490.7200\n",
      "31   46512.980469   50083.2000\n",
      "32   27197.722656   25521.1200\n",
      "33   53956.625000   52693.9200\n",
      "34   54435.523438   43636.3200\n",
      "35   37275.000000   35112.0528\n",
      "36   37889.378906   36709.9200\n",
      "37   58218.230469   58554.7200\n",
      "38   98078.578125   93080.1600\n",
      "39   34864.332031   38889.0720\n",
      "40   67558.117188   67239.3600\n",
      "41   80732.281250   87858.7200\n",
      "42   56918.218750   58341.6000\n",
      "43   78870.523438   87325.9200\n",
      "44   24427.824219   23389.9200\n",
      "45   14655.363281   15339.3120\n",
      "46   90610.054688  101178.7200\n",
      "47   63361.011719   45767.5200\n",
      "48   53732.246094   52161.1200\n",
      "49   31127.615234   34045.9200\n"
     ]
    }
   ],
   "source": [
    "loss, mae = nn_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "predicted_y = [x[0] for x in y_scaler.inverse_transform(nn_model.predict(X_test))]\n",
    "y_test = [x[0] for x in y_scaler.inverse_transform(y_test)]\n",
    "r_square = r2_score(y_test, predicted_y)\n",
    "\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R Squared: {r_square}\")\n",
    "\n",
    "\n",
    "dataframe_values = pd.DataFrame({\n",
    "    \"Predicted\" : predicted_y,\n",
    "    \"Actual\" : y_test\n",
    "})\n",
    "\n",
    "print(dataframe_values.head(50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
